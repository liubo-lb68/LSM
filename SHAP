import os
import random

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, precision_recall_curve
import pydotplus
import xgboost as xgb


# plot setting
plt.rcParams.update({'font.size': 18})
matplotlib.rc('axes', titlesize=18)
plt.style.use(['science',])
plt.rcParams['font.serif'] = 'Times New Roman'

# data
df = pd.read_csv('lsm_all_value.csv', encoding='gbk', index_col=[0, 1]).astype(np.float32)
df_train, df_test = train_test_split(df,test_size = 0.25, random_state=10)
df_train_columns = df_train.columns
X_columns = df.columns[:-1]

# XGBoost

dtrain = xgb.DMatrix(df_train[X_columns], label=df_train['target'], enable_categorical=True)
dtest = xgb.DMatrix(df_test[X_columns], label=df_test['target'], enable_categorical=True)

param = {'alpha': 0.08,
         'booster': 'gbtree',
         'colsample_bylevel': 0.7,
         'colsample_bynode': 0.7,
         'colsample_bytree': 1,
         'gamma': 0.03,
         'gpu_id': -1,
         'interaction_constraints': '',
         'learning_rate': 0.01,
         'max_delta_step': 3,
         'max_depth': 2,
         'min_child_weight': 10,
         'monotone_constraints': '()',
         'n_jobs': 2,
         'num_parallel_tree': 31,
         'random_state': 7,
         'reg_lambda': 0.74,
         'subsample': 0.75,
         'tree_method': 'exact',
         'validate_parameters': 1,
         'scale_pos_weight':1,
         'verbosity': None,
         }
		 
# training
bst = xgb.train(param, dtrain)

pred_train = bst.predict(dtrain)
pred_train_label = np.int64(pred_train>=0.5)
f1_train = f1_score(df_train['target'], pred_train_label)

# testing
preds = bst.predict(dtest)  
auc_value = roc_auc_score(df_test['target'], preds)

fpr, tpr, thresholds_roc = roc_curve(dtest.get_label(), preds)
print('auc:{}'.format(auc_value))
preds_label = np.int64(preds >= 0.5)
recall_value = recall_score(dtest.get_label(), preds_label)
print('racall:{}'.format(recall_value))
precision_value = precision_score(dtest.get_label(), preds_label)
print('precision:{}'.format(precision_value))
f1 = f1_score(dtest.get_label(), preds_label)
print('f1:{}'.format(f1))
precision_list, recall_list, thresholds = precision_recall_curve(
    dtest.get_label(), preds)

auc_pr = average_precision_score(dtest.get_label(), preds)
print('prc:{}'.format(auc_pr))

xgb_precisions, xgb_recalls, xgb_fpr, xgb_tpr, xgb_prc, xgb_auc = precision_list, recall_list, fpr, tpr, auc_pr, auc_value

# Interpretation
import shap

ax = xgb.plot_importance(bst, importance_type='gain')
fig = ax.figure
fig.set_size_inches(10, 10)

# Initializing the interpreter
explainer = shap.TreeExplainer(model_sklearn,
                               model_output='probability',
                               feature_perturbation='interventional',
                               data=df_test[X_columns])  
shap.initjs() 

# Calculate the SHAP value of each feature for each sample
shap_values = explainer.shap_values(df_test[X_columns]) 

# 
fig_root = r'G:\文献\LSM\可解释性'
fig_type = 'landslide_sample'
for i in np.where(df_test['target']==1)[0]:
    shap.force_plot(explainer.expected_value,
                    shap_values[i, :],
                    df_test[X_columns].iloc[i],
                    matplotlib=True,
                    show=False,
                    #link='logit',
                   )
    figure_name = str(i)+'_'+'{:.2f}'.format(preds[i])
    figpath = os.path.join(fig_root, fig_type, figure_name + '.PNG')
    plt.savefig(figpath)

shap.force_plot(explainer.expected_value,
                shap_values[np.where(df_test['target'] == 1)[0]],
                df_test[X_columns].iloc[np.where(df_test['target'] == 1)[0]])
			
shap.force_plot(explainer.expected_value,
                shap_values[np.where(df_test['target'] == 0)[0]],
                df_test[X_columns].iloc[np.where(df_test['target'] == 0)[0]])
				
# summarize the effects of all the features
shap.summary_plot(shap_values, df_test[X_columns],)
shap.summary_plot(shap_values, df_test[X_columns], plot_type="bar")

shap_interaction_values = explainer.shap_interaction_values(df_test[X_columns])
shap.summary_plot(shap_interaction_values, df_test[X_columns])

# Feature Interpretation
fig_root = r'G:\文献\LSM\可解释性'
fig_type = 'dependence'
for item in X_columns:
    shap.dependence_plot(item,
                         shap_values,
                         df_test[X_columns],
                         interaction_index=None,
                         show=False)
    figpath = os.path.join(fig_root, fig_type, item + '.PNG')
    plt.savefig(figpath)
	
fig_root = r'G:\文献\LSM\可解释性'
fig_type = 'Interaction'
for i in range(len(X_columns)):
    for j in range(len(X_columns)):
        # print(X_columns[i] + "_" + X_columns[j])
        col_i = X_columns[i]
        col_j = X_columns[j]
        shap.dependence_plot(col_i,
                             shap_values,
                             df_test[X_columns],
                             interaction_index=col_j,
                             show=False)
        filename = col_i + '_' + col_j
        figpath = os.path.join(fig_root, fig_type, filename + '.PNG')
        plt.savefig(figpath)
		
		
		
# Random Forest
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=91,
                             random_state=7,
                             max_depth=13,
                             min_samples_leaf=1,
                             min_samples_split=2)

rfc.fit(df_train[X_columns], df_train['target'])  # 训练

rf_pred_train = rfc.predict_proba(df_train[X_columns])[:,1]
rf_pred_train_label = rfc.predict(df_train[X_columns])
rf_f1_train = f1_score(df_train['target'], rf_pred_train_label)
print('train_f1:{}'.format(f1_train))

rf_preds = rfc.predict_proba(df_test[X_columns])[:,1]  # 预测
rf_auc_value = roc_auc_score(df_test['target'], rf_preds)

rf_fpr, rf_tpr, rf_thresholds_roc = roc_curve(df_test['target'], rf_preds)
print('auc:{}'.format(rf_auc_value))
rf_preds_label = rfc.predict(df_test[X_columns])
rf_recall_value = recall_score(df_test['target'], rf_preds_label)
print('racall:{}'.format(rf_recall_value))
rf_precision_value = precision_score(df_test['target'], rf_preds_label)
print('precision:{}'.format(rf_precision_value))
rf_f1 = f1_score(df_test['target'], rf_preds_label)
print('f1:{}'.format(rf_f1))
rf_precision_list, rf_recall_list, rf_thresholds = precision_recall_curve(
    df_test['target'], rf_preds)

rf_auc_pr = average_precision_score(df_test['target'], rf_preds)
print('prc:{}'.format(rf_auc_pr))

# LightGBM
import lightgbm as lgb
lgb_train = lgb.Dataset(df_train[X_columns],
                        label=df_train['target'],
                        categorical_feature=[
                            'Aspect', 'Land use', 'Distance from fault',
                            'Distance from road'
                        ])
lgb_test = lgb.Dataset(df_test[X_columns],
                       label=df_test['target'],
                       categorical_feature=[
                           'Aspect', 'Land use', 'Distance from fault',
                           'Distance from road'
                       ])
					   
			
model = lgb.LGBMClassifier(boosting_type='gbdt',
                           objective='binary',
                           metrics='auc',
                           learning_rate=0.01,
                           n_estimators=940,
                           max_depth=6,
                           num_leaves=45,
                           max_bin=175,
                           min_data_in_leaf=31,
                           bagging_fraction=0.9,
                           bagging_freq=20,
                           feature_fraction=0.9,
                           lambda_l1=1e-05,
                           lambda_l2=0.5,
                           min_split_gain=0)
model.fit(df_train[X_columns], df_train['target'])
y_pre = model.predict(df_test[X_columns])
# print("acc:", accuracy_score(df_test[X_columns], y_pre))
print("auc:", roc_auc_score(df_test['target'], y_pre))

pred_train = model.predict_proba(df_train[X_columns])[:,1]
pred_train_label = np.int64(pred_train >= 0.5)
f1_train = f1_score(df_train['target'], pred_train_label)
print('train_f1:{}'.format(f1_train))

preds = model.predict_proba(df_test[X_columns])[:,1]  # 预测
lgb_auc_value = roc_auc_score(df_test['target'], preds)

lgb_fpr, lgb_tpr, thresholds_roc = roc_curve(df_test['target'], preds)
print('auc:{}'.format(lgb_auc_value))
preds_label = np.int64(preds >= 0.5)
recall_value = recall_score(df_test['target'], preds_label)
print('racall:{}'.format(recall_value))
precision_value = precision_score(df_test['target'], preds_label)
print('precision:{}'.format(precision_value))
f1 = f1_score(df_test['target'], preds_label)
print('f1:{}'.format(f1))
lgb_precision_list, lgb_recall_list, thresholds = precision_recall_curve(
    df_test['target'], preds)

lgb_auc_pr = average_precision_score(df_test['target'], preds)
print('prc:{}'.format(auc_pr))

# Receiver Operating Characteristic curve
plt.figure()
lw = 2
plt.figure(figsize=(10, 10))
plt.plot(xgb_fpr,
         xgb_tpr,
         color='darkblue',
         lw=lw,
         label='XGBoost (area = %0.2f)' %
         auc_value)  # 假正率为横坐标，真正率为纵坐标做曲线

plt.plot(rf_fpr,
         rf_tpr,
         color='darkgreen',
         lw=lw,
         label='Random Forest (area = %0.2f)' %
         rf_auc_value)  # 假正率为横坐标，真正率为纵坐标做曲线

plt.plot(lgb_fpr,
         lgb_tpr,
         color='darkorange',
         lw=lw,
         label='LightGBM (area = %0.2f)' %
         lgb_auc_value)  # 假正率为横坐标，真正率为纵坐标做曲线

plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic curve')
plt.legend(loc="lower right")
plt.show()

# Precision recall curve
plt.figure()
lw = 2
plt.figure(figsize=(10, 10))
plt.plot(xgb_recalls,
         xgb_precisions,
         color='darkblue',
         lw=lw,
         label='XGBoost (area = %0.2f)' %
         xgb_prc)  # 假正率为横坐标，真正率为纵坐标做曲线

plt.plot(rf_recall_list,
         rf_precision_list,
         color='darkgreen',
         lw=lw,
         label='Random Forest (area = %0.2f)' %
         rf_auc_pr)  # 假正率为横坐标，真正率为纵坐标做曲线

plt.plot(lgb_recall_list,
         lgb_precision_list,
         color='darkorange',
         lw=lw,
         label='LighGBM (area = %0.2f)' %
         lgb_auc_pr)  # 假正率为横坐标，真正率为纵坐标做曲线

# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision recall curve')
plt.legend(loc="lower right")
plt.show()
